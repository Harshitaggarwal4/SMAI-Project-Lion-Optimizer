# Optimization-Methods-in-Neural-Networks
Reimplementation and exploration of the Lion optimization method, as described in the original paper "Lion," and its comparative analysis with several widely-used optimization algorithms, including AdamW, Stochastic Gradient Descent (SGD) with momentum, Nesterov Accelerated Gradient (Nesterov AG), AdaGrad, and RMSProp.

Blog Link : https://medium.com/@yash9439/lion-optimizer-73d3fd18abe9
