{"cells":[{"cell_type":"markdown","metadata":{"id":"glYHP4-7Jp--"},"source":["# Image Classification using Convolutional Neural Networks in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"kQxJbIfeJp-_"},"source":["## Exploring the CIFAR10 Dataset\n","\n","For this tutorial, we'll use the CIFAR10 dataset, which consists of 60000 32x32 px colour images in 10 classes. Here are some sample images from the dataset:\n","\n","<img src=\"https://miro.medium.com/max/709/1*LyV7_xga4jUHdx4_jHk1PQ.png\" style=\"max-width:480px\">\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:25.487482Z","iopub.status.busy":"2023-11-23T18:04:25.487200Z","iopub.status.idle":"2023-11-23T18:04:27.075156Z","shell.execute_reply":"2023-11-23T18:04:27.074411Z","shell.execute_reply.started":"2023-11-23T18:04:25.487456Z"},"executionInfo":{"elapsed":926,"status":"ok","timestamp":1607747714882,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"TkOhVp1FJp_A","trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torchvision\n","import tarfile\n","from torchvision.datasets.utils import download_url\n","from torch.utils.data import random_split"]},{"cell_type":"markdown","metadata":{"id":"VkBn_XvbJp_A"},"source":["We'll download the images in PNG format from [this page](https://course.fast.ai/datasets), using some helper functions from the `torchvision` and `tarfile` packages."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:27.077603Z","iopub.status.busy":"2023-11-23T18:04:27.077231Z","iopub.status.idle":"2023-11-23T18:04:32.046509Z","shell.execute_reply":"2023-11-23T18:04:32.045667Z","shell.execute_reply.started":"2023-11-23T18:04:27.077565Z"},"executionInfo":{"elapsed":3595,"status":"ok","timestamp":1607747188100,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"xazKUTC4Jp_A","outputId":"4cf34923-9c85-4fd7-d5bd-8b21089ebbe8","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz to ./cifar10.tgz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 135107811/135107811 [01:02<00:00, 2170061.86it/s]\n"]}],"source":["# Dowload the dataset\n","dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n","download_url(dataset_url, '.')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:32.048651Z","iopub.status.busy":"2023-11-23T18:04:32.048281Z","iopub.status.idle":"2023-11-23T18:04:47.410328Z","shell.execute_reply":"2023-11-23T18:04:47.409366Z","shell.execute_reply.started":"2023-11-23T18:04:32.048612Z"},"executionInfo":{"elapsed":14913,"status":"ok","timestamp":1607747201049,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"LF_8NbHiJp_C","outputId":"9eb3aa33-7e58-4928-e6b4-dde0904739a3","trusted":true},"outputs":[],"source":["# Extract from archive\n","with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n","    tar.extractall(path='./data')"]},{"cell_type":"markdown","metadata":{"id":"LJ86QfBlJp_C"},"source":["The dataset is extracted to the directory `data/cifar10`. It contains 2 folders `train` and `test`, containing the training set (50000 images) and test set (10000 images) respectively. Each of them contains 10 folders, one for each class of images. Let's verify this using `os.listdir`."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.411847Z","iopub.status.busy":"2023-11-23T18:04:47.411538Z","iopub.status.idle":"2023-11-23T18:04:47.417362Z","shell.execute_reply":"2023-11-23T18:04:47.416477Z","shell.execute_reply.started":"2023-11-23T18:04:47.411817Z"},"executionInfo":{"elapsed":10084,"status":"ok","timestamp":1607747201050,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"Ewt8o-hiJp_C","outputId":"8ac1e3bc-8fe0-4185-9ff2-1e34ab0778df","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['train', 'test']\n","['ship', 'frog', 'automobile', 'dog', 'cat', 'truck', 'deer', 'airplane', 'bird', 'horse']\n"]}],"source":["data_dir = './data/cifar10'\n","\n","print(os.listdir(data_dir))\n","classes = os.listdir(data_dir + \"/train\")\n","print(classes)"]},{"cell_type":"markdown","metadata":{"id":"9tzHkgTDJp_C"},"source":["Let's look inside a couple of folders, one from the training set and another from the test set. As an exercise, you can verify that that there are an equal number of images for each class, 5000 in the training set and 1000 in the test set."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.418750Z","iopub.status.busy":"2023-11-23T18:04:47.418484Z","iopub.status.idle":"2023-11-23T18:04:47.434226Z","shell.execute_reply":"2023-11-23T18:04:47.433324Z","shell.execute_reply.started":"2023-11-23T18:04:47.418724Z"},"executionInfo":{"elapsed":1168,"status":"ok","timestamp":1607747204422,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"sZS3sDJ4Jp_D","outputId":"0cc8475e-ab64-47cf-cd5c-e1c8e5abffd2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No. of training examples for airplanes: 5000\n","['4421.png', '4978.png', '3368.png', '4703.png', '3680.png']\n"]}],"source":["airplane_files = os.listdir(data_dir + \"/train/airplane\")\n","print('No. of training examples for airplanes:', len(airplane_files))\n","print(airplane_files[:5])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.435661Z","iopub.status.busy":"2023-11-23T18:04:47.435288Z","iopub.status.idle":"2023-11-23T18:04:47.441974Z","shell.execute_reply":"2023-11-23T18:04:47.441099Z","shell.execute_reply.started":"2023-11-23T18:04:47.435633Z"},"executionInfo":{"elapsed":1639,"status":"ok","timestamp":1607747204914,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"rvhUgwAMJp_D","outputId":"f1a2538f-0bdb-43af-db90-16343d3e01ad","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No. of test examples for ship: 1000\n","['0470.png', '0126.png', '0690.png', '0158.png', '0263.png']\n"]}],"source":["ship_test_files = os.listdir(data_dir + \"/test/ship\")\n","print(\"No. of test examples for ship:\", len(ship_test_files))\n","print(ship_test_files[:5])"]},{"cell_type":"markdown","metadata":{"id":"wF5kv2I3Jp_D"},"source":["The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the `ImageFolder` class from `torchvision` to load the data as PyTorch tensors."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.443434Z","iopub.status.busy":"2023-11-23T18:04:47.443161Z","iopub.status.idle":"2023-11-23T18:04:47.451159Z","shell.execute_reply":"2023-11-23T18:04:47.450321Z","shell.execute_reply.started":"2023-11-23T18:04:47.443408Z"},"executionInfo":{"elapsed":1086,"status":"ok","timestamp":1607747204915,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"ubxtJzJjJp_E","trusted":true},"outputs":[],"source":["from torchvision.datasets import ImageFolder\n","from torchvision.transforms import ToTensor"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.453871Z","iopub.status.busy":"2023-11-23T18:04:47.453587Z","iopub.status.idle":"2023-11-23T18:04:47.787995Z","shell.execute_reply":"2023-11-23T18:04:47.787081Z","shell.execute_reply.started":"2023-11-23T18:04:47.453846Z"},"executionInfo":{"elapsed":1325,"status":"ok","timestamp":1607747205682,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"6IAxD0e9Jp_E","trusted":true},"outputs":[],"source":["dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"]},{"cell_type":"markdown","metadata":{"id":"ilbPijY3Jp_F"},"source":["Let's look at a sample element from the training dataset. Each element is a tuple, containing a image tensor and a label. Since the data consists of 32x32 px color images with 3 channels (RGB), each image tensor has the shape `(3, 32, 32)`."]},{"cell_type":"markdown","metadata":{"id":"KylRsYg2Jp_F"},"source":["The list of classes is stored in the `.classes` property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.873371Z","iopub.status.busy":"2023-11-23T18:04:47.873082Z","iopub.status.idle":"2023-11-23T18:04:47.877824Z","shell.execute_reply":"2023-11-23T18:04:47.876798Z","shell.execute_reply.started":"2023-11-23T18:04:47.873343Z"},"executionInfo":{"elapsed":1119,"status":"ok","timestamp":1607747208442,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"O-XOdz6KJp_G","outputId":"899cf040-7274-4ba2-e477-bb7799ed8419","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"]}],"source":["print(dataset.classes)"]},{"cell_type":"markdown","metadata":{"id":"UwAIns3lJp_G"},"source":["We can view the image using `matplotlib`, but we need to change the tensor dimensions to `(32,32,3)`. Let's create a helper function to display an image and its label."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.879283Z","iopub.status.busy":"2023-11-23T18:04:47.878999Z","iopub.status.idle":"2023-11-23T18:04:47.890309Z","shell.execute_reply":"2023-11-23T18:04:47.889569Z","shell.execute_reply.started":"2023-11-23T18:04:47.879256Z"},"executionInfo":{"elapsed":1169,"status":"ok","timestamp":1607747743034,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"JC8sRIySO187","trusted":true},"outputs":[],"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","matplotlib.rcParams['figure.facecolor'] = '#ffffff'"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.891743Z","iopub.status.busy":"2023-11-23T18:04:47.891447Z","iopub.status.idle":"2023-11-23T18:04:47.901325Z","shell.execute_reply":"2023-11-23T18:04:47.900547Z","shell.execute_reply.started":"2023-11-23T18:04:47.891716Z"},"executionInfo":{"elapsed":787,"status":"ok","timestamp":1607747208957,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"6TM5cZVyJp_G","trusted":true},"outputs":[],"source":["\n","def show_example(img, label):\n","    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n","    plt.imshow(img.permute(1, 2, 0))"]},{"cell_type":"markdown","metadata":{"id":"P_Fhylr7Jp_H"},"source":["Let's look at a couple of images from the dataset. As you can tell, the 32x32px images are quite difficult to identify, even for the human eye. Try changing the indices below to view different images."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:47.902636Z","iopub.status.busy":"2023-11-23T18:04:47.902373Z","iopub.status.idle":"2023-11-23T18:04:48.080199Z","shell.execute_reply":"2023-11-23T18:04:48.079350Z","shell.execute_reply.started":"2023-11-23T18:04:47.902610Z"},"executionInfo":{"elapsed":1306,"status":"ok","timestamp":1607747210554,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"DLd1lDT0Jp_H","outputId":"4d9d45bc-7adf-419d-c60d-eb09644f4f08","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Label:  airplane (0)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["show_example(*dataset[0])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:48.081618Z","iopub.status.busy":"2023-11-23T18:04:48.081338Z","iopub.status.idle":"2023-11-23T18:04:48.230845Z","shell.execute_reply":"2023-11-23T18:04:48.229868Z","shell.execute_reply.started":"2023-11-23T18:04:48.081588Z"},"executionInfo":{"elapsed":1263,"status":"ok","timestamp":1607747210999,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"C-bLFBZ0Jp_H","outputId":"5deb9a4c-7c5c-48c1-c726-822051a979f1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Label:  airplane (0)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["show_example(*dataset[1099])"]},{"cell_type":"markdown","metadata":{"id":"QxuW5ucGJp_J"},"source":["## Training and Validation Datasets\n","\n","While building real world machine learning models, it is quite common to split the dataset into 3 parts:\n","\n","1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n","2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n","3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n","\n","Since there's no predefined validation set, we can set aside a small portion (5000 images) of the training set to be used as the validation set. We'll use the `random_split` helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:48.232269Z","iopub.status.busy":"2023-11-23T18:04:48.232005Z","iopub.status.idle":"2023-11-23T18:04:48.238512Z","shell.execute_reply":"2023-11-23T18:04:48.237314Z","shell.execute_reply.started":"2023-11-23T18:04:48.232243Z"},"executionInfo":{"elapsed":984,"status":"ok","timestamp":1607747283986,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"MSVGdRK3Jp_J","trusted":true},"outputs":[],"source":["random_seed = 42\n","torch.manual_seed(random_seed);"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:48.239898Z","iopub.status.busy":"2023-11-23T18:04:48.239606Z","iopub.status.idle":"2023-11-23T18:04:48.256518Z","shell.execute_reply":"2023-11-23T18:04:48.255691Z","shell.execute_reply.started":"2023-11-23T18:04:48.239870Z"},"executionInfo":{"elapsed":653,"status":"ok","timestamp":1607747285214,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"yy5ekvZOJp_L","outputId":"47493e92-c6d8-4838-aa96-4c68f13aa9bc","trusted":true},"outputs":[{"data":{"text/plain":["(45000, 5000)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["val_size = 5000\n","train_size = len(dataset) - val_size\n","\n","train_ds, val_ds = random_split(dataset, [train_size, val_size])\n","len(train_ds), len(val_ds)"]},{"cell_type":"markdown","metadata":{"id":"s8Hu-d6mJp_M"},"source":["We can now create data loaders for training and validation, to load the data in batches"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:48.258158Z","iopub.status.busy":"2023-11-23T18:04:48.257745Z","iopub.status.idle":"2023-11-23T18:04:48.262008Z","shell.execute_reply":"2023-11-23T18:04:48.261158Z","shell.execute_reply.started":"2023-11-23T18:04:48.258115Z"},"executionInfo":{"elapsed":731,"status":"ok","timestamp":1607747288038,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"7wM8WQFfJp_M","trusted":true},"outputs":[],"source":["from torch.utils.data.dataloader import DataLoader\n","\n","batch_size=128"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:48.263410Z","iopub.status.busy":"2023-11-23T18:04:48.263141Z","iopub.status.idle":"2023-11-23T18:04:48.271788Z","shell.execute_reply":"2023-11-23T18:04:48.270991Z","shell.execute_reply.started":"2023-11-23T18:04:48.263381Z"},"executionInfo":{"elapsed":749,"status":"ok","timestamp":1607747288519,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"lWXX_hHGJp_M","trusted":true},"outputs":[],"source":["batch_size=128\n","train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","val_dl = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)"]},{"cell_type":"markdown","metadata":{"id":"0YM8SZq7Jp_M"},"source":["We can look at batches of images from the dataset using the `make_grid` method from `torchvision`. Each time the following code is run, we get a different bach, since the sampler shuffles the indices before creating batches."]},{"cell_type":"markdown","metadata":{"id":"PkRCd9f4Jp_O"},"source":["## Defining the Model (Convolutional Neural Network)\n","\n","For this tutorial, we will use a convolutional neural network, using the `nn.Conv2d` class from PyTorch.\n","\n","> The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel. - [Source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n","\n","<img src=\"https://miro.medium.com/max/1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" style=\"max-width:400px;\">\n","\n","\n","Let us implement a convolution operation on a 1 channel image with a 3x3 kernel."]},{"cell_type":"markdown","metadata":{"id":"JZFVHVT7Jp_P"},"source":["For multi-channel images, a different kernel is applied to each channels, and the outputs are added together pixel-wise. \n","\n","Checking out the following articles to gain a better understanding of convolutions:\n","\n","1. [Intuitively understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) by Irhum Shafkat\n","2. [Convolutions in Depth](https://sgugger.github.io/convolution-in-depth.html) by Sylvian Gugger (this article implements convolutions from scratch)\n","\n","There are certain advantages offered by convolutional layers when working with image data:\n","\n","* **Fewer parameters**: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer. \n","* **Sparsity of connections**: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient.\n","* **Parameter sharing and spatial invariance**: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image.\n","\n","We will also use a [max-pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) layers to progressively decrease the height & width of the output tensors from each convolutional layer.\n","\n","<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" style=\"max-width:400px;\">\n","\n","Before we define the entire model, let's look at how a single convolutional layer followed by a max-pooling layer operates on the data."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:53.420079Z","iopub.status.busy":"2023-11-23T18:04:53.419710Z","iopub.status.idle":"2023-11-23T18:04:53.424486Z","shell.execute_reply":"2023-11-23T18:04:53.423481Z","shell.execute_reply.started":"2023-11-23T18:04:53.420046Z"},"executionInfo":{"elapsed":669,"status":"ok","timestamp":1607747335570,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"CKaWyJtUJp_P","trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"WSKyZ2xIJp_P"},"source":["The `Conv2d` layer transforms a 3-channel image to a 16-channel *feature map*, and the `MaxPool2d` layer halves the height and width. The feature map gets smaller as we add more layers, until we are finally left with a small feature map, which can be flattened into a vector. We can then add some fully connected layers at the end to get vector of size 10 for each image.\n","\n","<img src=\"https://i.imgur.com/KKtPOKE.png\" style=\"max-width:540px\">\n","\n","Let's define the model by extending an `ImageClassificationBase` class which contains helper methods for training & validation."]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:53.841949Z","iopub.status.busy":"2023-11-23T18:04:53.841634Z","iopub.status.idle":"2023-11-23T18:04:53.852801Z","shell.execute_reply":"2023-11-23T18:04:53.851639Z","shell.execute_reply.started":"2023-11-23T18:04:53.841915Z"},"executionInfo":{"elapsed":1420,"status":"ok","timestamp":1607747364977,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"WgbZ6gLOJp_Q","trusted":true},"outputs":[],"source":["class ImageClassificationBase(nn.Module):\n","    def training_step(self, batch):\n","        images, labels = batch \n","        out = self(images)                  # Generate predictions\n","        loss = F.cross_entropy(out, labels) # Calculate loss\n","        return loss\n","    \n","    def validation_step(self, batch):\n","        images, labels = batch \n","        out = self(images)                    # Generate predictions\n","        loss = F.cross_entropy(out, labels)   # Calculate loss\n","        acc = accuracy(out, labels)           # Calculate accuracy\n","        return {'val_loss': loss.detach(), 'val_acc': acc}\n","        \n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n","        batch_accs = [x['val_acc'] for x in outputs]\n","        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n","        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n","    \n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n","        \n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"]},{"cell_type":"markdown","metadata":{"id":"rw-AyZcjJp_Q"},"source":["\n","We'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture."]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:53.854913Z","iopub.status.busy":"2023-11-23T18:04:53.854613Z","iopub.status.idle":"2023-11-23T18:04:53.866694Z","shell.execute_reply":"2023-11-23T18:04:53.865818Z","shell.execute_reply.started":"2023-11-23T18:04:53.854886Z"},"executionInfo":{"elapsed":1018,"status":"ok","timestamp":1607747376351,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"F4xrvghlJp_Q","trusted":true},"outputs":[],"source":["class Cifar10CnnModel(ImageClassificationBase):\n","    def __init__(self):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n","\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n","\n","            nn.Flatten(), \n","            nn.Linear(256*4*4, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10))\n","        \n","    def forward(self, xb):\n","        return self.network(xb)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:53.868210Z","iopub.status.busy":"2023-11-23T18:04:53.867936Z","iopub.status.idle":"2023-11-23T18:04:53.930217Z","shell.execute_reply":"2023-11-23T18:04:53.929426Z","shell.execute_reply.started":"2023-11-23T18:04:53.868183Z"},"executionInfo":{"elapsed":618,"status":"ok","timestamp":1607747377900,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"tU5q48KeJp_Q","outputId":"4fda5779-6db2-4516-8616-f874e193866b","trusted":true},"outputs":[{"data":{"text/plain":["Cifar10CnnModel(\n","  (network): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU()\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU()\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU()\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU()\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU()\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): Flatten(start_dim=1, end_dim=-1)\n","    (16): Linear(in_features=4096, out_features=1024, bias=True)\n","    (17): ReLU()\n","    (18): Linear(in_features=1024, out_features=512, bias=True)\n","    (19): ReLU()\n","    (20): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["model = Cifar10CnnModel()\n","model"]},{"cell_type":"markdown","metadata":{"id":"loJN3i6_Jp_Q"},"source":["Let's verify that the model produces the expected output on a batch of training data. The 10 outputs for each image can be interpreted as probabilities for the 10 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:53.931782Z","iopub.status.busy":"2023-11-23T18:04:53.931404Z","iopub.status.idle":"2023-11-23T18:04:54.579156Z","shell.execute_reply":"2023-11-23T18:04:54.577968Z","shell.execute_reply.started":"2023-11-23T18:04:53.931727Z"},"executionInfo":{"elapsed":2212,"status":"ok","timestamp":1607747386801,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"YCm2I82pJp_Q","outputId":"98685680-7bbb-436f-9e45-cb5f348c494f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["images.shape: torch.Size([128, 3, 32, 32])\n","out.shape: torch.Size([128, 10])\n","out[0]: tensor([ 0.0239, -0.0466,  0.0067,  0.0193,  0.0044, -0.0598, -0.0188, -0.0242,\n","         0.0431, -0.0164], grad_fn=<SelectBackward0>)\n"]}],"source":["for images, labels in train_dl:\n","    print('images.shape:', images.shape)\n","    out = model(images)\n","    print('out.shape:', out.shape)\n","    print('out[0]:', out[0])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"oASDWWc9Jp_R"},"source":["To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required. "]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:54.581042Z","iopub.status.busy":"2023-11-23T18:04:54.580713Z","iopub.status.idle":"2023-11-23T18:04:54.589518Z","shell.execute_reply":"2023-11-23T18:04:54.588682Z","shell.execute_reply.started":"2023-11-23T18:04:54.581010Z"},"executionInfo":{"elapsed":1180,"status":"ok","timestamp":1607747389383,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"wR9oVOopJp_R","trusted":true},"outputs":[],"source":["def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","    \n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)"]},{"cell_type":"markdown","metadata":{"id":"5zBM9zOqJp_R"},"source":["Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:54.590900Z","iopub.status.busy":"2023-11-23T18:04:54.590584Z","iopub.status.idle":"2023-11-23T18:04:54.609180Z","shell.execute_reply":"2023-11-23T18:04:54.607750Z","shell.execute_reply.started":"2023-11-23T18:04:54.590874Z"},"executionInfo":{"elapsed":600,"status":"ok","timestamp":1607747390986,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"iczDAm1SJp_R","outputId":"e473803f-beac-4300-bd62-f3cd5dd9de84","trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["device = get_default_device()\n","device"]},{"cell_type":"markdown","metadata":{"id":"zdlOEla3Jp_R"},"source":["We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available)."]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:54.611094Z","iopub.status.busy":"2023-11-23T18:04:54.610675Z","iopub.status.idle":"2023-11-23T18:04:54.631055Z","shell.execute_reply":"2023-11-23T18:04:54.630332Z","shell.execute_reply.started":"2023-11-23T18:04:54.611051Z"},"executionInfo":{"elapsed":968,"status":"ok","timestamp":1607747393203,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"vx3BnG8IJp_R","trusted":true},"outputs":[],"source":["train_dl = DeviceDataLoader(train_dl, device)\n","val_dl = DeviceDataLoader(val_dl, device)\n","to_device(model, device);"]},{"cell_type":"markdown","metadata":{"id":"0qq2eMlEJp_S"},"source":["## Model Training Functions\n","\n","We'll define two functions: `fit` and `evaluate` to train the model using gradient descent and evaluate its performance on the validation set. "]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:04:54.633138Z","iopub.status.busy":"2023-11-23T18:04:54.632529Z","iopub.status.idle":"2023-11-23T18:04:54.646117Z","shell.execute_reply":"2023-11-23T18:04:54.644333Z","shell.execute_reply.started":"2023-11-23T18:04:54.633095Z"},"executionInfo":{"elapsed":1069,"status":"ok","timestamp":1607747502883,"user":{"displayName":"Aakash N S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64","userId":"03254185060287524023"},"user_tz":-330},"id":"aEQkLCaYJp_S","trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model, val_loader):\n","    model.eval()\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    return model.validation_epoch_end(outputs)\n","\n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n","\n","def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n","    history = []\n","    optimizer = opt_func(model.parameters(), lr)\n","    for epoch in range(epochs):\n","        # Training Phase \n","        model.train()\n","        train_losses = []\n","        train_accuracies = []\n","        for batch in train_loader:\n","            loss = model.training_step(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            # Calculate accuracy\n","            output = model(batch[0])  # Assuming batch[0] is the input data\n","            acc = accuracy(output, batch[1])  # Assuming batch[1] are the labels\n","            train_accuracies.append(acc)\n","\n","        # Validation phase\n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        result['train_acc'] = torch.stack(train_accuracies).mean().item()  # Add train accuracy\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","    return history\n"]},{"cell_type":"markdown","metadata":{"id":"mGbR0ClAJp_S"},"source":["Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters."]},{"cell_type":"markdown","metadata":{"id":"WEuauDpwJp_S"},"source":["The initial accuracy is around 10%, which is what one might expect from a randomly intialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n","\n","We'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model. As an exercise, you can try changing these to see if you have achieve a higher accuracy in a shorter time. "]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:22:00.295920Z","iopub.status.busy":"2023-11-23T18:22:00.295566Z","iopub.status.idle":"2023-11-23T18:22:07.803144Z","shell.execute_reply":"2023-11-23T18:22:07.802254Z","shell.execute_reply.started":"2023-11-23T18:22:00.295887Z"},"trusted":true},"outputs":[],"source":["# !pip install lion-pytorch"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T18:24:08.316388Z","iopub.status.busy":"2023-11-23T18:24:08.316035Z","iopub.status.idle":"2023-11-23T18:24:08.327078Z","shell.execute_reply":"2023-11-23T18:24:08.325885Z","shell.execute_reply.started":"2023-11-23T18:24:08.316356Z"},"trusted":true},"outputs":[],"source":["class lion_optimizer(torch.optim.Optimizer): \n","\n","    def __init__(self, params, learning_rate = 1e-3, beta1 = 0.9, beta2 = 0.99, lambdaa = 0.01):\n","        super(lion_optimizer, self).__init__(params, defaults={'lr': learning_rate})\n","        self.state = dict()\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.lambdaa = lambdaa\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                self.state[p] = dict(momentum = torch.zeros_like(p.data))\n","\n","    def step(self): \n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p not in self.state: \n","                    self.state[p] = dict(momentum = torch.zeros_like(p.data))\n","                m = self.beta1 * self.state[p]['momentum'] + (1 - self.beta1) * p.grad.data\n","                p.data -= group['lr'] * (torch.sign(m) + self.lambdaa * p.data)\n","                m = self.beta2 * self.state[p]['momentum'] + (1 - self.beta2) * p.grad.data\n","                self.state[p]['momentum'] = m"]},{"cell_type":"markdown","metadata":{"id":"55K2kRe6Jp_T"},"source":["We can also plot the valdation set accuracies to study how the model improves over time."]},{"cell_type":"markdown","metadata":{},"source":["# Batch Size Ablation"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["batch_sizes = [32, 64, 128, 256, 512, 1024, 2048, 4096]"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/8 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch [0], train_loss: 1.6292, val_loss: 1.2310, val_acc: 0.5410\n","Epoch [1], train_loss: 1.0575, val_loss: 0.9178, val_acc: 0.6722\n","Epoch [2], train_loss: 0.8067, val_loss: 0.7562, val_acc: 0.7387\n","Epoch [3], train_loss: 0.6427, val_loss: 0.7178, val_acc: 0.7550\n","Epoch [4], train_loss: 0.5224, val_loss: 0.6625, val_acc: 0.7769\n","Epoch [5], train_loss: 0.4243, val_loss: 0.6594, val_acc: 0.7876\n","Epoch [6], train_loss: 0.4111, val_loss: 0.7067, val_acc: 0.7783\n","Epoch [7], train_loss: 0.3783, val_loss: 0.6995, val_acc: 0.7864\n","Epoch [8], train_loss: 0.2763, val_loss: 0.7565, val_acc: 0.7818\n","Epoch [9], train_loss: 0.2778, val_loss: 0.8174, val_acc: 0.7797\n","Epoch [10], train_loss: 0.2135, val_loss: 0.9186, val_acc: 0.7743\n","Epoch [11], train_loss: 0.1837, val_loss: 0.9897, val_acc: 0.7753\n","Epoch [12], train_loss: 0.1866, val_loss: 0.9872, val_acc: 0.7735\n","Epoch [13], train_loss: 0.1606, val_loss: 1.0439, val_acc: 0.7783\n","Epoch [14], train_loss: 0.3093, val_loss: 0.9582, val_acc: 0.7633\n","Epoch [15], train_loss: 0.3901, val_loss: 0.8800, val_acc: 0.7743\n","Epoch [16], train_loss: 0.1944, val_loss: 0.9647, val_acc: 0.7659\n","Epoch [17], train_loss: 0.1812, val_loss: 0.9542, val_acc: 0.7542\n","Epoch [18], train_loss: 0.1629, val_loss: 1.0050, val_acc: 0.7858\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▎        | 1/8 [06:29<45:28, 389.83s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch [19], train_loss: 0.1304, val_loss: 1.1106, val_acc: 0.7771\n","Epoch [0], train_loss: 1.7786, val_loss: 1.4719, val_acc: 0.4591\n","Epoch [1], train_loss: 1.2571, val_loss: 1.1063, val_acc: 0.6068\n","Epoch [2], train_loss: 0.9694, val_loss: 0.9213, val_acc: 0.6768\n","Epoch [3], train_loss: 0.7683, val_loss: 0.7753, val_acc: 0.7334\n","Epoch [4], train_loss: 0.6209, val_loss: 0.6832, val_acc: 0.7654\n","Epoch [5], train_loss: 0.5050, val_loss: 0.6950, val_acc: 0.7712\n","Epoch [6], train_loss: 0.4054, val_loss: 0.6536, val_acc: 0.7840\n","Epoch [7], train_loss: 0.3176, val_loss: 0.6739, val_acc: 0.7903\n","Epoch [8], train_loss: 0.2455, val_loss: 0.7058, val_acc: 0.7864\n","Epoch [9], train_loss: 0.1950, val_loss: 0.8031, val_acc: 0.7937\n","Epoch [10], train_loss: 0.1609, val_loss: 0.8324, val_acc: 0.7963\n","Epoch [11], train_loss: 0.1488, val_loss: 0.8629, val_acc: 0.7933\n","Epoch [12], train_loss: 0.1309, val_loss: 0.8668, val_acc: 0.7814\n","Epoch [13], train_loss: 0.1199, val_loss: 1.0127, val_acc: 0.7896\n","Epoch [14], train_loss: 0.1200, val_loss: 0.9377, val_acc: 0.7949\n","Epoch [15], train_loss: 0.1082, val_loss: 1.0446, val_acc: 0.7791\n","Epoch [16], train_loss: 0.1169, val_loss: 1.0479, val_acc: 0.7820\n","Epoch [17], train_loss: 0.1407, val_loss: 0.9992, val_acc: 0.7811\n","Epoch [18], train_loss: 0.0992, val_loss: 1.0840, val_acc: 0.7917\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 2/8 [11:54<35:10, 351.78s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch [19], train_loss: 0.1716, val_loss: 0.9317, val_acc: 0.7795\n","Epoch [0], train_loss: 1.9138, val_loss: 1.6042, val_acc: 0.4018\n","Epoch [1], train_loss: 1.4437, val_loss: 1.2855, val_acc: 0.5271\n","Epoch [2], train_loss: 1.1624, val_loss: 1.0666, val_acc: 0.6129\n","Epoch [3], train_loss: 0.9622, val_loss: 0.9000, val_acc: 0.6873\n","Epoch [4], train_loss: 0.7944, val_loss: 0.7843, val_acc: 0.7213\n","Epoch [5], train_loss: 0.6579, val_loss: 0.7497, val_acc: 0.7420\n","Epoch [6], train_loss: 0.5389, val_loss: 0.6774, val_acc: 0.7645\n","Epoch [7], train_loss: 0.4274, val_loss: 0.6593, val_acc: 0.7811\n","Epoch [8], train_loss: 0.3254, val_loss: 0.6943, val_acc: 0.7844\n","Epoch [9], train_loss: 0.2384, val_loss: 0.7495, val_acc: 0.7904\n","Epoch [10], train_loss: 0.1741, val_loss: 0.7953, val_acc: 0.7912\n","Epoch [11], train_loss: 0.1320, val_loss: 0.8875, val_acc: 0.7930\n","Epoch [12], train_loss: 0.1115, val_loss: 0.9461, val_acc: 0.7877\n","Epoch [13], train_loss: 0.1012, val_loss: 0.9578, val_acc: 0.7869\n","Epoch [14], train_loss: 0.0938, val_loss: 1.0273, val_acc: 0.7775\n","Epoch [15], train_loss: 0.0868, val_loss: 1.0311, val_acc: 0.7865\n","Epoch [16], train_loss: 0.0827, val_loss: 1.0245, val_acc: 0.7902\n","Epoch [17], train_loss: 0.0733, val_loss: 1.1056, val_acc: 0.7846\n","Epoch [18], train_loss: 0.0743, val_loss: 1.0991, val_acc: 0.7979\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 3/8 [16:26<26:15, 315.08s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch [19], train_loss: 0.0702, val_loss: 1.0569, val_acc: 0.7939\n","Epoch [0], train_loss: 2.0828, val_loss: 1.9135, val_acc: 0.2983\n","Epoch [1], train_loss: 1.7441, val_loss: 1.6095, val_acc: 0.4028\n","Epoch [2], train_loss: 1.4981, val_loss: 1.4184, val_acc: 0.4820\n","Epoch [3], train_loss: 1.3232, val_loss: 1.2567, val_acc: 0.5430\n","Epoch [4], train_loss: 1.1656, val_loss: 1.1125, val_acc: 0.5961\n","Epoch [5], train_loss: 1.0247, val_loss: 1.0301, val_acc: 0.6277\n","Epoch [6], train_loss: 0.9041, val_loss: 0.9058, val_acc: 0.6751\n","Epoch [7], train_loss: 0.7906, val_loss: 0.8296, val_acc: 0.7051\n","Epoch [8], train_loss: 0.6837, val_loss: 0.7772, val_acc: 0.7234\n","Epoch [9], train_loss: 0.5917, val_loss: 0.7505, val_acc: 0.7460\n","Epoch [10], train_loss: 0.5062, val_loss: 0.6963, val_acc: 0.7614\n","Epoch [11], train_loss: 0.4252, val_loss: 0.7392, val_acc: 0.7679\n","Epoch [12], train_loss: 0.3433, val_loss: 0.7619, val_acc: 0.7624\n","Epoch [13], train_loss: 0.2718, val_loss: 0.7521, val_acc: 0.7796\n","Epoch [14], train_loss: 0.2052, val_loss: 0.8348, val_acc: 0.7773\n","Epoch [15], train_loss: 0.1509, val_loss: 0.9303, val_acc: 0.7790\n","Epoch [16], train_loss: 0.1108, val_loss: 1.0513, val_acc: 0.7750\n","Epoch [17], train_loss: 0.0898, val_loss: 1.0923, val_acc: 0.7741\n","Epoch [18], train_loss: 0.0793, val_loss: 1.1350, val_acc: 0.7793\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 4/8 [20:52<19:42, 295.69s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch [19], train_loss: 0.0745, val_loss: 1.1977, val_acc: 0.7772\n","Epoch [0], train_loss: 2.1779, val_loss: 2.0487, val_acc: 0.2433\n","Epoch [1], train_loss: 1.9793, val_loss: 1.9199, val_acc: 0.2923\n","Epoch [2], train_loss: 1.8254, val_loss: 1.7603, val_acc: 0.3479\n","Epoch [3], train_loss: 1.6625, val_loss: 1.6169, val_acc: 0.4016\n","Epoch [4], train_loss: 1.5392, val_loss: 1.4939, val_acc: 0.4563\n","Epoch [5], train_loss: 1.4415, val_loss: 1.4317, val_acc: 0.4806\n","Epoch [6], train_loss: 1.3553, val_loss: 1.3224, val_acc: 0.5175\n","Epoch [7], train_loss: 1.2759, val_loss: 1.2727, val_acc: 0.5399\n","Epoch [8], train_loss: 1.1953, val_loss: 1.1849, val_acc: 0.5683\n","Epoch [9], train_loss: 1.1163, val_loss: 1.1128, val_acc: 0.6018\n","Epoch [10], train_loss: 1.0438, val_loss: 1.0621, val_acc: 0.6204\n","Epoch [11], train_loss: 0.9735, val_loss: 1.0334, val_acc: 0.6306\n","Epoch [12], train_loss: 0.9072, val_loss: 0.9691, val_acc: 0.6549\n","Epoch [13], train_loss: 0.8460, val_loss: 0.9284, val_acc: 0.6704\n","Epoch [14], train_loss: 0.7847, val_loss: 0.8834, val_acc: 0.6878\n","Epoch [15], train_loss: 0.7253, val_loss: 0.8764, val_acc: 0.6955\n","Epoch [16], train_loss: 0.6656, val_loss: 0.8349, val_acc: 0.7102\n","Epoch [17], train_loss: 0.6046, val_loss: 0.8200, val_acc: 0.7176\n","Epoch [18], train_loss: 0.5514, val_loss: 0.7997, val_acc: 0.7332\n"]},{"name":"stderr","output_type":"stream","text":[" 62%|██████▎   | 5/8 [24:37<13:30, 270.20s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch [19], train_loss: 0.4973, val_loss: 0.7916, val_acc: 0.7380\n","Epoch [0], train_loss: 2.2391, val_loss: 2.1391, val_acc: 0.2177\n","Epoch [1], train_loss: 2.0691, val_loss: 2.0274, val_acc: 0.2464\n","Epoch [2], train_loss: 1.9571, val_loss: 1.9102, val_acc: 0.3057\n","Epoch [3], train_loss: 1.8591, val_loss: 1.8186, val_acc: 0.3326\n","Epoch [4], train_loss: 1.7661, val_loss: 1.7406, val_acc: 0.3563\n","Epoch [5], train_loss: 1.6802, val_loss: 1.6546, val_acc: 0.3845\n","Epoch [6], train_loss: 1.6046, val_loss: 1.5897, val_acc: 0.4161\n","Epoch [7], train_loss: 1.5396, val_loss: 1.5290, val_acc: 0.4334\n","Epoch [8], train_loss: 1.4827, val_loss: 1.4730, val_acc: 0.4569\n","Epoch [9], train_loss: 1.4257, val_loss: 1.4224, val_acc: 0.4782\n","Epoch [10], train_loss: 1.3760, val_loss: 1.3647, val_acc: 0.4952\n","Epoch [11], train_loss: 1.3273, val_loss: 1.3239, val_acc: 0.5111\n","Epoch [12], train_loss: 1.2768, val_loss: 1.2645, val_acc: 0.5404\n","Epoch [13], train_loss: 1.2234, val_loss: 1.2346, val_acc: 0.5445\n","Epoch [14], train_loss: 1.1733, val_loss: 1.1870, val_acc: 0.5662\n","Epoch [15], train_loss: 1.1323, val_loss: 1.1554, val_acc: 0.5835\n","Epoch [16], train_loss: 1.0868, val_loss: 1.0983, val_acc: 0.6037\n","Epoch [17], train_loss: 1.0453, val_loss: 1.0781, val_acc: 0.6158\n","Epoch [18], train_loss: 0.9975, val_loss: 1.0446, val_acc: 0.6313\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 6/8 [28:21<08:28, 254.42s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch [19], train_loss: 0.9571, val_loss: 1.0177, val_acc: 0.6364\n","Epoch [0], train_loss: 2.2914, val_loss: 2.2445, val_acc: 0.1582\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 6/8 [28:35<09:31, 285.90s/it]\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 7.77 GiB total capacity; 3.64 GiB already allocated; 565.94 MiB free; 5.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33741/2460886886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCifar10CnnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlion_batchSize_ablation_train_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33741/3711227051.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 7.77 GiB total capacity; 3.64 GiB already allocated; 565.94 MiB free; 5.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["from tqdm import tqdm\n","lion_batchSize_ablation_train_loss = {}\n","lion_batchSize_ablation_valid_loss = {}\n","lion_batchSize_ablation_train_accuracy = {}\n","lion_batchSize_ablation_valid_accuracy = {}\n","\n","for batch_size in tqdm(batch_sizes):\n","    train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","    val_dl = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)\n","    train_dl = DeviceDataLoader(train_dl, device)\n","    val_dl = DeviceDataLoader(val_dl, device)\n","    to_device(model, device)\n","\n","    num_epochs = 20\n","    opt_func = lion_optimizer\n","    lr = 1e-4\n","    model = to_device(Cifar10CnnModel(), device)\n","    history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n","\n","    lion_batchSize_ablation_train_loss[batch_size] = []\n","    lion_batchSize_ablation_valid_loss[batch_size] = []\n","    lion_batchSize_ablation_train_accuracy[batch_size] = []\n","    lion_batchSize_ablation_valid_accuracy[batch_size] = []\n","\n","    for epoch_history in history:\n","        lion_batchSize_ablation_train_loss[batch_size].append(epoch_history['train_loss'])\n","        lion_batchSize_ablation_valid_loss[batch_size].append(epoch_history['val_loss'])\n","        lion_batchSize_ablation_train_accuracy[batch_size].append(epoch_history['train_acc'])\n","        lion_batchSize_ablation_valid_accuracy[batch_size].append(epoch_history['val_acc'])"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["[{'val_loss': 2.139134407043457,\n","  'val_acc': 0.21773886680603027,\n","  'train_loss': 2.239062786102295,\n","  'train_acc': 0.15807069838047028},\n"," {'val_loss': 2.0273900032043457,\n","  'val_acc': 0.2463962286710739,\n","  'train_loss': 2.069089412689209,\n","  'train_acc': 0.23685401678085327},\n"," {'val_loss': 1.9102083444595337,\n","  'val_acc': 0.3056986331939697,\n","  'train_loss': 1.9571280479431152,\n","  'train_acc': 0.28979089856147766},\n"," {'val_loss': 1.8185739517211914,\n","  'val_acc': 0.33256879448890686,\n","  'train_loss': 1.8590906858444214,\n","  'train_acc': 0.32587289810180664},\n"," {'val_loss': 1.7406208515167236,\n","  'val_acc': 0.3563312292098999,\n","  'train_loss': 1.7660993337631226,\n","  'train_acc': 0.3532344102859497},\n"," {'val_loss': 1.6545885801315308,\n","  'val_acc': 0.38448041677474976,\n","  'train_loss': 1.680193305015564,\n","  'train_acc': 0.3809789717197418},\n"," {'val_loss': 1.589747667312622,\n","  'val_acc': 0.4161055088043213,\n","  'train_loss': 1.6045799255371094,\n","  'train_acc': 0.4078269302845001},\n"," {'val_loss': 1.5290287733078003,\n","  'val_acc': 0.43343472480773926,\n","  'train_loss': 1.5396236181259155,\n","  'train_acc': 0.4355756640434265},\n"," {'val_loss': 1.473049283027649,\n","  'val_acc': 0.4568946957588196,\n","  'train_loss': 1.4826685190200806,\n","  'train_acc': 0.4562986493110657},\n"," {'val_loss': 1.4223833084106445,\n","  'val_acc': 0.4782218039035797,\n","  'train_loss': 1.4256712198257446,\n","  'train_acc': 0.47646763920783997},\n"," {'val_loss': 1.3646658658981323,\n","  'val_acc': 0.49517250061035156,\n","  'train_loss': 1.3759777545928955,\n","  'train_acc': 0.49951303005218506},\n"," {'val_loss': 1.3239202499389648,\n","  'val_acc': 0.5111085772514343,\n","  'train_loss': 1.3272596597671509,\n","  'train_acc': 0.5188261270523071},\n"," {'val_loss': 1.2645429372787476,\n","  'val_acc': 0.5403864979743958,\n","  'train_loss': 1.2768007516860962,\n","  'train_acc': 0.538981020450592},\n"," {'val_loss': 1.234610915184021,\n","  'val_acc': 0.5444638133049011,\n","  'train_loss': 1.2234407663345337,\n","  'train_acc': 0.5574911832809448},\n"," {'val_loss': 1.1869858503341675,\n","  'val_acc': 0.5662195682525635,\n","  'train_loss': 1.173335075378418,\n","  'train_acc': 0.5789797306060791},\n"," {'val_loss': 1.1554358005523682,\n","  'val_acc': 0.5835107564926147,\n","  'train_loss': 1.132309079170227,\n","  'train_acc': 0.5929999947547913},\n"," {'val_loss': 1.0983095169067383,\n","  'val_acc': 0.6036504507064819,\n","  'train_loss': 1.0868159532546997,\n","  'train_acc': 0.6113255620002747},\n"," {'val_loss': 1.0781114101409912,\n","  'val_acc': 0.6158237457275391,\n","  'train_loss': 1.0453341007232666,\n","  'train_acc': 0.629050076007843},\n"," {'val_loss': 1.0445845127105713,\n","  'val_acc': 0.6313295364379883,\n","  'train_loss': 0.9974984526634216,\n","  'train_acc': 0.6452198624610901},\n"," {'val_loss': 1.0177408456802368,\n","  'val_acc': 0.6364352703094482,\n","  'train_loss': 0.9571143388748169,\n","  'train_acc': 0.6611273288726807}]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["history"]},{"cell_type":"markdown","metadata":{},"source":["# Saving Results"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["import json\n","\n","# Save the dictionary to a file\n","with open('lion_modelArchirecture_ablation_train_loss.json', 'w') as file:\n","    json.dump(lion_batchSize_ablation_train_loss, file)\n","\n","\n","# Save the dictionary to a file\n","with open('lion_batchSize_ablation_valid_loss.json', 'w') as file:\n","    json.dump(lion_batchSize_ablation_valid_loss, file)\n","\n","\n","# Save the dictionary to a file\n","with open('lion_batchSize_ablation_train_accuracy.json', 'w') as file:\n","    json.dump(lion_batchSize_ablation_train_accuracy, file)\n","\n","\n","# Save the dictionary to a file\n","with open('lion_batchSize_ablation_valid_accuracy.json', 'w') as file:\n","    json.dump(lion_batchSize_ablation_valid_accuracy, file)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30121,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
