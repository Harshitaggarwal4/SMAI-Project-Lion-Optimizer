{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_image(src_image, size=(128, 128), bg_color=(255, 255, 255)):\n",
    "    # Resize the image while maintaining its aspect ratio\n",
    "    h, w, _ = src_image.shape\n",
    "    aspect_ratio = w / h\n",
    "    new_w = size[0]\n",
    "    new_h = int(new_w / aspect_ratio)\n",
    "    if new_h > size[1]:\n",
    "        new_h = size[1]\n",
    "        new_w = int(new_h * aspect_ratio)\n",
    "\n",
    "    resized_image = cv2.resize(src_image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Create a new square background image\n",
    "    new_image = np.full((size[1], size[0], 3), bg_color, dtype=np.uint8)\n",
    "\n",
    "    # Calculate the position to paste the resized image in the center\n",
    "    x_offset = (size[0] - new_w) // 2\n",
    "    y_offset = (size[1] - new_h) // 2\n",
    "\n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized_image\n",
    "\n",
    "    return new_image\n",
    "\n",
    "def resize_images_in_folder(input_folder, output_folder, size=(128, 128)):\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "\n",
    "    for root, folders, files in os.walk(input_folder):\n",
    "        for sub_folder in folders:\n",
    "            # Create a matching subfolder in the output directory\n",
    "            save_folder = os.path.join(output_folder, sub_folder)\n",
    "            if not os.path.exists(save_folder):\n",
    "                os.makedirs(save_folder)\n",
    "            # Loop through the files in the subfolder\n",
    "            file_names = os.listdir(os.path.join(root, sub_folder))\n",
    "            for file_name in file_names:\n",
    "                # Open the file\n",
    "                file_path = os.path.join(root, sub_folder, file_name)\n",
    "                image = cv2.imread(file_path)\n",
    "                # Create a resized version and save it\n",
    "                resized_image = resize_image(image, size)\n",
    "                save_as = os.path.join(save_folder, file_name)\n",
    "                cv2.imwrite(save_as, resized_image)\n",
    "\n",
    "# Set the paths\n",
    "input_images_path = 'natural_images'\n",
    "resized_images_path = 'resized/natural_images'\n",
    "\n",
    "# Define the image size\n",
    "img_size = (128, 128)\n",
    "classes = [\"airplane\", \"car\", \"cat\", \"dog\", \"flower\", \"fruit\", \"motorbike\", \"person\"]\n",
    "\n",
    "# Call the function to resize images\n",
    "resize_images_in_folder(input_images_path, resized_images_path, size=img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to resize images\n",
    "resize_images_in_folder(input_images_path, resized_images_path, size=img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_transform_images(data_path, batch_size=64, train_fraction=0.8, num_workers=0):\n",
    "    # Define data transformations\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5),  # Randomly flip horizontally\n",
    "        transforms.RandomVerticalFlip(0.3),    # Randomly flip vertically\n",
    "        transforms.ToTensor(),                 # Convert images to tensors\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
    "    ])\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = datasets.ImageFolder(root=data_path, transform=data_transform)\n",
    "\n",
    "    # Calculate the sizes for training and testing sets\n",
    "    total_samples = len(dataset)\n",
    "    train_size = int(train_fraction * total_samples)\n",
    "    test_size = total_samples - train_size\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    random_seed = 42\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=1 - train_fraction, random_state=random_seed)\n",
    "\n",
    "    # Split the training dataset into train and dev sets\n",
    "    train_size = len(train_dataset)\n",
    "    dev_size = int(train_size * 0.1)  # Adjust the fraction as needed\n",
    "    train_size -= dev_size\n",
    "\n",
    "    train_dataset, dev_dataset = data.random_split(train_dataset, [train_size, dev_size])\n",
    "\n",
    "    # Create data loaders for the training, dev, and testing data\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    dev_loader = data.DataLoader(dev_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the resized dataset\n",
    "train_loader, dev_loader, test_loader = load_and_transform_images(resized_images_path)\n",
    "batch_size = train_loader.batch_size  # You can get the batch size from the train_loader|\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # Define layers in the neural network\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network layers\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        x = self.fc(x)\n",
    "        return torch.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=24576, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine if a GPU is available for training\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model and allocate it to the device\n",
    "model = Net(num_classes=len(classes)).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train(model, device, train_loader, optimizer, loss_criteria, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    # print(\"Epoch:\", epoch)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_criteria(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct += torch.sum(target == predicted).item()\n",
    "        # print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "    \n",
    "    avg_loss = train_loss / (batch_idx + 1)\n",
    "    # print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Function to test the model\n",
    "def test(model, device, test_loader, loss_criteria):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        \n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target == predicted).item()\n",
    "    \n",
    "    avg_loss = test_loss / batch_count\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    # print('Testing set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)'.format(avg_loss, correct, len(test_loader.dataset), accuracy))\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Function to test the model\n",
    "def valid(model, device, dev_loader, loss_criteria):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        \n",
    "        for data, target in dev_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target == predicted).item()\n",
    "    \n",
    "    avg_loss = test_loss / batch_count\n",
    "    accuracy = 100. * correct / len(dev_loader.dataset)\n",
    "    # print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)'.format(avg_loss, correct, len(dev_loader.dataset), accuracy))\n",
    "    \n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class momentum_optimizer(torch.optim.Optimizer): \n",
    "\n",
    "    def __init__(self, params, learning_rate = 1e-3, beta = 0.9): \n",
    "        super(momentum_optimizer, self).__init__(params, defaults={'lr': learning_rate}) \n",
    "        self.beta = beta \n",
    "        self.state = dict() \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p] = dict(momentum = torch.zeros_like(p.data)) \n",
    "\n",
    "    def step(self): \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p not in self.state: \n",
    "                    self.state[p] = dict(momentum = torch.zeros_like(p.data))\n",
    "                v = self.beta * self.state[p]['momentum'] + group['lr'] * p.grad.data\n",
    "                p.data -= v\n",
    "                self.state[p]['momentum'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nag_optimizer(torch.optim.Optimizer): \n",
    "\n",
    "    def __init__(self, params, learning_rate = 1e-3, beta = 0.9): \n",
    "        super(nag_optimizer, self).__init__(params, defaults={'lr': learning_rate}) \n",
    "        self.beta = beta \n",
    "        self.state = dict() \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p] = dict(momentum = torch.zeros_like(p.data)) \n",
    "\n",
    "    def step(self): \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p not in self.state: \n",
    "                    self.state[p] = dict(momentum = torch.zeros_like(p.data))\n",
    "                v = self.beta * (1 - group['lr']) * self.state[p]['momentum'] + group['lr'] * p.grad.data\n",
    "                p.data -= v\n",
    "                self.state[p]['momentum'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rms_prop_optimizer(torch.optim.Optimizer): \n",
    "\n",
    "    def __init__(self, params, learning_rate = 1e-3, epsilon = 1e-7, beta = 0.95):\n",
    "        super(rms_prop_optimizer, self).__init__(params, defaults={'lr': learning_rate}) \n",
    "        self.epsilon = epsilon \n",
    "        self.state = dict()\n",
    "        self.beta = beta\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p] = dict(lr_update = torch.zeros_like(p.data))\n",
    "\n",
    "    def step(self): \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p not in self.state: \n",
    "                    self.state[p] = dict(lr_update = torch.zeros_like(p.data))\n",
    "                v = self.beta * self.state[p]['lr_update'] + (1 - self.beta) * (p.grad.data)**2\n",
    "                p.data -= group['lr'] * p.grad.data / torch.sqrt(v + self.epsilon)\n",
    "                self.state[p]['lr_update'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adam_optimizer(torch.optim.Optimizer): \n",
    "\n",
    "    def __init__(self, params, learning_rate = 1e-3, epsilon = 1e-7, beta1 = 0.9, beta2 = 0.999):\n",
    "        super(adam_optimizer, self).__init__(params, defaults={'lr': learning_rate}) \n",
    "        self.epsilon = epsilon \n",
    "        self.state = dict()\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p] = dict(lr_update = torch.zeros_like(p.data), momentum = torch.zeros_like(p.data), t = 0)\n",
    "\n",
    "    def step(self): \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p not in self.state: \n",
    "                    self.state[p] = dict(lr_update = torch.zeros_like(p.data), momentum = torch.zeros_like(p.data), t = 0)\n",
    "                self.state[p]['t'] += 1\n",
    "                v = self.beta2 * self.state[p]['lr_update'] + (1 - self.beta2) * (p.grad.data)**2\n",
    "                m = self.beta1 * self.state[p]['momentum'] + (1 - self.beta1) * p.grad.data\n",
    "                self.state[p]['lr_update'] = v\n",
    "                self.state[p]['momentum'] = m\n",
    "                m = m / (1 - self.beta1 ** self.state[p]['t'])\n",
    "                v = v / (1 - self.beta2 ** self.state[p]['t'])\n",
    "                p.data -= group['lr'] * m / torch.sqrt(v + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adam_w_optimizer(torch.optim.Optimizer): \n",
    "\n",
    "    def __init__(self, params, learning_rate = 1e-3, epsilon = 1e-7, beta1 = 0.9, beta2 = 0.999, lambdaa = 0.01):\n",
    "        super(adam_w_optimizer, self).__init__(params, defaults={'lr': learning_rate}) \n",
    "        self.epsilon = epsilon \n",
    "        self.state = dict()\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.lambdaa = lambdaa\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p] = dict(lr_update = torch.zeros_like(p.data), momentum = torch.zeros_like(p.data), t = 0)\n",
    "\n",
    "    def step(self): \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p not in self.state: \n",
    "                    self.state[p] = dict(lr_update = torch.zeros_like(p.data), momentum = torch.zeros_like(p.data), t = 0)\n",
    "                self.state[p]['t'] += 1\n",
    "                v = self.beta2 * self.state[p]['lr_update'] + (1 - self.beta2) * (p.grad.data)**2\n",
    "                m = self.beta1 * self.state[p]['momentum'] + (1 - self.beta1) * p.grad.data\n",
    "                self.state[p]['lr_update'] = v\n",
    "                self.state[p]['momentum'] = m\n",
    "                m = m / (1 - self.beta1 ** self.state[p]['t'])\n",
    "                v = v / (1 - self.beta2 ** self.state[p]['t'])\n",
    "                p.data -= group['lr'] * (m / torch.sqrt(v + self.epsilon) + self.lambdaa * p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lion_optimizer(torch.optim.Optimizer): \n",
    "\n",
    "    def __init__(self, params, learning_rate = 1e-3, beta1 = 0.9, beta2 = 0.99, lambdaa = 0.01):\n",
    "        super(lion_optimizer, self).__init__(params, defaults={'lr': learning_rate})\n",
    "        self.state = dict()\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.lambdaa = lambdaa\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p] = dict(momentum = torch.zeros_like(p.data))\n",
    "\n",
    "    def step(self): \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p not in self.state: \n",
    "                    self.state[p] = dict(momentum = torch.zeros_like(p.data))\n",
    "                m = self.beta1 * self.state[p]['momentum'] + (1 - self.beta1) * p.grad.data\n",
    "                p.data -= group['lr'] * (torch.sign(m) + self.lambdaa * p.data)\n",
    "                m = self.beta2 * self.state[p]['momentum'] + (1 - self.beta2) * p.grad.data\n",
    "                self.state[p]['momentum'] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, optimizer, loss_criteria, train_loader, dev_loader, test_loader, epochs):\n",
    "    # Track metrics in these arrays\n",
    "    epoch_nums = []\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    testing_loss = []\n",
    "    training_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    testing_accuracy = []\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        test_loss, test_accuracy = test(model, device, test_loader, loss_criteria)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        testing_loss.append(test_loss)\n",
    "        validation_loss.append(val_loss)\n",
    "        training_accuracy.append(train_accuracy)\n",
    "        validation_accuracy.append(test_accuracy)\n",
    "        testing_accuracy.append(val_accuracy)\n",
    "\n",
    "    return training_loss, validation_loss, testing_loss, training_accuracy, validation_accuracy, testing_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.02\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1)))\n",
    "momentum_momentum_ablation_train_loss = {}\n",
    "momentum_momentum_ablation_valid_loss = {}\n",
    "momentum_momentum_ablation_train_accuracy = {}\n",
    "momentum_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = momentum_optimizer(model.parameters(), learning_rate=lr, beta=beta)\n",
    "    \n",
    "    momentum_momentum_ablation_train_loss[beta] = []\n",
    "    momentum_momentum_ablation_valid_loss[beta] = []\n",
    "    momentum_momentum_ablation_train_accuracy[beta] = []\n",
    "    momentum_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        momentum_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        momentum_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        momentum_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        momentum_momentum_ablation_valid_accuracy[beta].append(val_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('momentum_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(momentum_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('momentum_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(momentum_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('momentum_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(momentum_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('momentum_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(momentum_momentum_ablation_valid_accuracy, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAG Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1)))\n",
    "nag_momentum_ablation_train_loss = {}\n",
    "nag_momentum_ablation_valid_loss = {}\n",
    "nag_momentum_ablation_train_accuracy = {}\n",
    "nag_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = nag_optimizer(model.parameters(), learning_rate=lr, beta=beta)\n",
    "    \n",
    "    nag_momentum_ablation_train_loss[beta] = []\n",
    "    nag_momentum_ablation_valid_loss[beta] = []\n",
    "    nag_momentum_ablation_train_accuracy[beta] = []\n",
    "    nag_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        nag_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        nag_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        nag_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        nag_momentum_ablation_valid_accuracy[beta].append(val_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('nag_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(nag_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('nag_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(nag_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('nag_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(nag_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('nag_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(nag_momentum_ablation_valid_accuracy, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1)))\n",
    "\n",
    "epsilon = 1e-7\n",
    "rmsprop_momentum_ablation_train_loss = {}\n",
    "rmsprop_momentum_ablation_valid_loss = {}\n",
    "rmsprop_momentum_ablation_train_accuracy = {}\n",
    "rmsprop_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = rms_prop_optimizer(model.parameters(), learning_rate=lr, epsilon=epsilon, beta=beta)\n",
    "    \n",
    "    rmsprop_momentum_ablation_train_loss[beta] = []\n",
    "    rmsprop_momentum_ablation_valid_loss[beta] = []\n",
    "    rmsprop_momentum_ablation_train_accuracy[beta] = []\n",
    "    rmsprop_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        rmsprop_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        rmsprop_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        rmsprop_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        rmsprop_momentum_ablation_valid_accuracy[beta].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('rmsprop_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(rmsprop_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('rmsprop_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(rmsprop_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('rmsprop_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(rmsprop_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('rmsprop_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(rmsprop_momentum_ablation_valid_accuracy, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1), np.arange(0.1, 1.1, 0.1)))\n",
    "\n",
    "epsilon = 0.000001\n",
    "beta1 = 0.9\n",
    "\n",
    "adam_beta2_momentum_ablation_train_loss = {}\n",
    "adam_beta2_momentum_ablation_valid_loss = {}\n",
    "adam_beta2_momentum_ablation_train_accuracy = {}\n",
    "adam_beta2_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = adam_optimizer(model.parameters(), learning_rate=lr, epsilon=epsilon, beta1=beta1, beta2=beta)\n",
    "    \n",
    "    adam_beta2_momentum_ablation_train_loss[beta] = []\n",
    "    adam_beta2_momentum_ablation_valid_loss[beta] = []\n",
    "    adam_beta2_momentum_ablation_train_accuracy[beta] = []\n",
    "    adam_beta2_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        adam_beta2_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        adam_beta2_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        adam_beta2_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        adam_beta2_momentum_ablation_valid_accuracy[beta].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_beta2_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(adam_beta2_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_beta2_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(adam_beta2_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_beta2_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(adam_beta2_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_beta2_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(adam_beta2_momentum_ablation_valid_accuracy, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1), np.arange(0.1, 1.1, 0.1)))\n",
    "\n",
    "epsilon = 0.000001\n",
    "beta2 = 0.999\n",
    "\n",
    "adam_beta1_momentum_ablation_train_loss = {}\n",
    "adam_beta1_momentum_ablation_valid_loss = {}\n",
    "adam_beta1_momentum_ablation_train_accuracy = {}\n",
    "adam_beta1_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = adam_optimizer(model.parameters(), learning_rate=lr, epsilon=epsilon, beta1=beta, beta2=beta2)\n",
    "    \n",
    "    adam_beta1_momentum_ablation_train_loss[beta] = []\n",
    "    adam_beta1_momentum_ablation_valid_loss[beta] = []\n",
    "    adam_beta1_momentum_ablation_train_accuracy[beta] = []\n",
    "    adam_beta1_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        adam_beta1_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        adam_beta1_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        adam_beta1_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        adam_beta1_momentum_ablation_valid_accuracy[beta].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_beta1_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(adam_beta1_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_beta1_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(adam_beta1_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_beta1_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(adam_beta1_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_beta1_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(adam_beta1_momentum_ablation_valid_accuracy, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1), np.arange(0.1, 1.1, 0.1)))\n",
    "\n",
    "epsilon = 0.000001\n",
    "beta1 = 0.9\n",
    "lambdaa = 0.005\n",
    "\n",
    "adam_w_beta2_momentum_ablation_train_loss = {}\n",
    "adam_w_beta2_momentum_ablation_valid_loss = {}\n",
    "adam_w_beta2_momentum_ablation_train_accuracy = {}\n",
    "adam_w_beta2_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = adam_w_optimizer(model.parameters(), learning_rate=lr, epsilon=epsilon, beta1=beta1, beta2=beta, lambdaa=lambdaa)\n",
    "    \n",
    "    adam_w_beta2_momentum_ablation_train_loss[beta] = []\n",
    "    adam_w_beta2_momentum_ablation_valid_loss[beta] = []\n",
    "    adam_w_beta2_momentum_ablation_train_accuracy[beta] = []\n",
    "    adam_w_beta2_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        adam_w_beta2_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        adam_w_beta2_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        adam_w_beta2_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        adam_w_beta2_momentum_ablation_valid_accuracy[beta].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_w_beta2_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(adam_w_beta2_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_w_beta2_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(adam_w_beta2_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_w_beta2_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(adam_w_beta2_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_w_beta2_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(adam_w_beta2_momentum_ablation_valid_accuracy, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1), np.arange(0.1, 1.1, 0.1)))\n",
    "\n",
    "epsilon = 0.000001\n",
    "beta2 = 0.999\n",
    "lambdaa = 0.005\n",
    "\n",
    "adam_w_beta1_momentum_ablation_train_loss = {}\n",
    "adam_w_beta1_momentum_ablation_valid_loss = {}\n",
    "adam_w_beta1_momentum_ablation_train_accuracy = {}\n",
    "adam_w_beta1_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = adam_w_optimizer(model.parameters(), learning_rate=lr, epsilon=epsilon, beta1=beta, beta2=beta2, lambdaa=lambdaa)\n",
    "    \n",
    "    adam_w_beta1_momentum_ablation_train_loss[beta] = []\n",
    "    adam_w_beta1_momentum_ablation_valid_loss[beta] = []\n",
    "    adam_w_beta1_momentum_ablation_train_accuracy[beta] = []\n",
    "    adam_w_beta1_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        adam_w_beta1_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        adam_w_beta1_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        adam_w_beta1_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        adam_w_beta1_momentum_ablation_valid_accuracy[beta].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_w_beta1_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(adam_w_beta1_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_w_beta1_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(adam_w_beta1_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_w_beta1_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(adam_w_beta1_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('adam_w_beta1_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(adam_w_beta1_momentum_ablation_valid_accuracy, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1), np.arange(0.1, 1.1, 0.1)))\n",
    "beta2 = 0.99\n",
    "lambdaa = 0.007\n",
    "\n",
    "lion_beta1_momentum_ablation_train_loss = {}\n",
    "lion_beta1_momentum_ablation_valid_loss = {}\n",
    "lion_beta1_momentum_ablation_train_accuracy = {}\n",
    "lion_beta1_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = lion_optimizer(model.parameters(), learning_rate=lr, beta1=beta, beta2=beta2, lambdaa=lambdaa)\n",
    "    \n",
    "    lion_beta1_momentum_ablation_train_loss[beta] = []\n",
    "    lion_beta1_momentum_ablation_valid_loss[beta] = []\n",
    "    lion_beta1_momentum_ablation_train_accuracy[beta] = []\n",
    "    lion_beta1_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        lion_beta1_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        lion_beta1_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        lion_beta1_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        lion_beta1_momentum_ablation_valid_accuracy[beta].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('lion_beta1_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(lion_beta1_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('lion_beta1_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(lion_beta1_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('lion_beta1_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(lion_beta1_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('lion_beta1_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(lion_beta1_momentum_ablation_valid_accuracy, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 80\n",
    "\n",
    "betas = np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 0.1, 0.01), np.arange(0.1, 1.1, 0.1), np.arange(0.1, 1.1, 0.1)))\n",
    "beta1 = 0.9\n",
    "lambdaa = 0.007\n",
    "\n",
    "lion_beta2_momentum_ablation_train_loss = {}\n",
    "lion_beta2_momentum_ablation_valid_loss = {}\n",
    "lion_beta2_momentum_ablation_train_accuracy = {}\n",
    "lion_beta2_momentum_ablation_valid_accuracy = {}\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "for beta in betas:\n",
    "    model = Net(num_classes=len(classes)).to(device)\n",
    "    optimizer = lion_optimizer(model.parameters(), learning_rate=lr, beta1=beta1, beta2=beta, lambdaa=lambdaa)\n",
    "    \n",
    "    lion_beta2_momentum_ablation_train_loss[beta] = []\n",
    "    lion_beta2_momentum_ablation_valid_loss[beta] = []\n",
    "    lion_beta2_momentum_ablation_train_accuracy[beta] = []\n",
    "    lion_beta2_momentum_ablation_valid_accuracy[beta] = []\n",
    "            \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_criteria, epoch)\n",
    "        val_loss, val_accuracy = valid(model, device, dev_loader, loss_criteria)\n",
    "\n",
    "        lion_beta2_momentum_ablation_train_loss[beta].append(train_loss)\n",
    "        lion_beta2_momentum_ablation_valid_loss[beta].append(val_loss)\n",
    "        lion_beta2_momentum_ablation_train_accuracy[beta].append(train_accuracy)\n",
    "        lion_beta2_momentum_ablation_valid_accuracy[beta].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('lion_beta2_momentum_ablation_train_loss.json', 'w') as file:\n",
    "    json.dump(lion_beta2_momentum_ablation_train_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('lion_beta2_momentum_ablation_valid_loss.json', 'w') as file:\n",
    "    json.dump(lion_beta2_momentum_ablation_valid_loss, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('lion_beta2_momentum_ablation_train_accuracy.json', 'w') as file:\n",
    "    json.dump(lion_beta2_momentum_ablation_train_accuracy, file)\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('lion_beta2_momentum_ablation_valid_accuracy.json', 'w') as file:\n",
    "    json.dump(lion_beta2_momentum_ablation_valid_accuracy, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
